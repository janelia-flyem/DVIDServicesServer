#!/usr/bin/env python

"""
Usage: prog <num spark nodes> <workflow name> <job id> <callback address> <spark services workflow script> <python location>

Action: Calls script that creates spark cluster and launches spark workflow

Assumptions:
    1. The environment should be properly set by the driver.
    2. spark_launch should be in the executable path
"""

try:
    import sys
    import os
    import drmaa

    # set up drmaa session
    s = drmaa.Session()
    s.initialize()
    jt = s.createJobTemplate()
    
    # (ADD CUSTOM NON-BLOCKING CALL TO LAUNCH SCRIPT ON SPARK MASTER!!)
    # use current environment and launch with number of machines (1 for master)
    
    jt.nativeSpecification = "-jc spark-rc -pe spark-rc " + str(int(sys.argv[1])+1) + " -q hadoop2 -j y -V -o " + sys.argv[3]+".wrapper.log" + " -m b" 
    jt.remoteCommand = "sparklaunch_janelia_int"

    # provides all parameters except the number of nodes
    jt.args = sys.argv[2:]

    # run job
    jobid = s.runJob(jt)
except Exception as e:
    print str(e)
    exit(-1)
