#!/usr/bin/env python

"""
Usage: prog <num spark nodes> <workflow name> <job id> <callback address> <spark services workflow script> <python location>

Action: Calls script that creates spark cluster and launches spark workflow

Assumptions:
    1. The environment should be properly set by the driver.
    2. spark_launch should be in the executable path
"""

USE_DRMAA = False

import sys
import os

# (ADD CUSTOM NON-BLOCKING CALL TO LAUNCH SCRIPT ON SPARK MASTER!!)
# use current environment and launch with number of machines (1 for master)

num_spark_nodes = int(sys.argv[1]) + 1 # Add one for master
job_name = sys.argv[3]

qsub_args = ( " -jc spark"                   # Spark job class
              " -pe spark {num_spark_nodes}" # Spark parallel environment
              " -q hadoop2"                  # Use Hadoop queue
              " -V"                          # Copy environment to nodes
              " -j y"                        # Join output and error logs
              " -o {job_name}.wrapper.log"   # Output log file
            ).format(**locals())

job_command = "sparklaunch_janelia_int"
extra_args = sys.argv[2:]

qsub_cmd = "qsub {qsub_args} {node_command} {cmd_args}"\
           .format( qsub_args=qsub_args, node_command=job_command, cmd_args=' '.join(extra_args) )

if USE_DRMAA:
    print "About to launch job with drmaa, equivalent to the following qsub command:"
    print qsub_cmd + "\n"

    import drmaa
    # set up drmaa session
    s = drmaa.Session()
    s.initialize()
    jt = s.createJobTemplate()
    jt.nativeSpecification = qsub_args
    jt.remoteCommand = job_command
    jt.args = extra_args # provides all parameters except the number of nodes
    jobid = s.runJob(jt)

else:
    print "About to launch job with qsub:"
    print qsub_cmd + "\n"
    
    import subprocess
    subprocess.check_call(qsub_cmd, shell=True)
