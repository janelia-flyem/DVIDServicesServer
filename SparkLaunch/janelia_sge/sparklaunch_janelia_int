#!/usr/bin/env python

"""
Usage: prog <workflow name> <job id> <callback address> <spark services workflow script> <python location>

If the callback address does not start with an 'http' it is assumed to be a configuration path and no callback will be used

Actions:
    1.  Provides spark callback address to launching server (via server callback)
    2.  Initializes spark cluster
    3.  Runs job on server (blocking)
    4.  Examines outputs for errors and informs callback

Assumptions: The environment should be properly set by the driver.
"""

##### CUSTOM CONFIG (global parameters -- change as needed!!) ####

# Location of spark distribution
SPARK_HOME = "/usr/local/spark-current"

# spark configuration path (disable default python)
CONF_DIR = "/groups/flyem/proj/cluster/miniconda/envs/flyem/DVIDSparkServices/conf"

# directory location of log file
LOG_DIR = "/groups/scheffer/home/plazas/sparkjoblogs"

##### CUSTOM CONFIGURATION ###########
    
import os
import socket
import sys
import subprocess
import time
import requests
import tempfile
import json

master_output = ""
successful = True

json_header = {'content-type': 'app/json'}
start = time.time()

hascallback = True
if not sys.argv[3].startswith("http"):
    hascallback = False
    
try:
    # start workflow script location
    WORKFLOW_LAUNCHER = sys.argv[4] 
    
    # Add directories to PATH
    PATH_DIRS = SPARK_HOME + "/bin:" + SPARK_HOME + "/sbin"

    # set path
    curr_path = os.environ["PATH"]
    os.environ["PATH"] = PATH_DIRS + ":" + curr_path

    # set spark path
    os.environ["SPARK_HOME"] = SPARK_HOME

    # set configuration directory
    os.environ["SPARK_CONF_DIR"] = CONF_DIR

    # set exact python to be used
    os.environ["PYSPARK_PYTHON"] = sys.argv[5] 

    # ******** Launch Spark Cluster *******
    HOSTNAME = socket.gethostname()

    master_output = subprocess.check_output([SPARK_HOME + '/sbin/start-master.sh'])
    
    # this sets default master -- does not need to be specified on commandline
    os.environ["MASTER"] = "spark://" + HOSTNAME + ":7077"

    master_output = master_output + "\n" + os.environ["MASTER"]

    # ******** Start Job ********
    configfile = sys.argv[3]

    if hascallback:
        # write-back callback address
        status = {}
        status["sparkAddr"] = HOSTNAME
        status["job_status"] = "Running"
        status_str = json.dumps(status)

        requests.post(sys.argv[3], data=status_str, headers=json_header)

        configfile = configfile + "/config"

    # wait a few seconds for the cluster to stabilise (is this necessary?)
    time.sleep(10)

    # call workflow and wait
    try:
        job_output = subprocess.check_output([SPARK_HOME + '/bin/spark-submit', WORKFLOW_LAUNCHER, sys.argv[1], '-c', configfile])
    except Exception as e:
        job_output += str(sys.exc_info()[0])
        successful = False
   
    master_output = master_output + "\n" + job_output
except Exception as e:
    master_output += str(sys.exc_info()[0])
    successful = False

# record time
finish = time.time()
master_output += "\n" + "Total Time: " + str(finish-start)

# write status and message
status = {}
status["sparkAddr"] = ""
if not successful:
    status["job_status"] = "Error"
    status["job_message"] = master_output
else:
    status["job_status"] = "Finished"
    status["job_message"] = master_output
status_str = json.dumps(status)

if hascallback:
    requests.post(sys.argv[3], data=status_str, headers=json_header)

# write to file
fout = open(LOG_DIR + "/" + sys.argv[2]+'.log', 'w')
fout.write(master_output)

