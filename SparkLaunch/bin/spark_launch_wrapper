#!/usr/bin/env python

"""
Usage: prog <num spark nodes> <server callback> <python workflow script> <workflow config file>

Action: Calls script that creates spark cluster and launches spark workflow

Assumptions: The environment should be properly set by the driver.
"""

import drmaa
import sys

s = drmaa.Session()
s.initialize()
jt = s.createJobTemplate()

# use current environment and launch with number of machines (16 cores) + 1 for master
jt.nativeSpecification = "-jc spark -pe spark " + str(int(sys.argv[1])+1) + " -q hadoop2 -j y -o ~/.spark/ugelogs -V -m b-j y -o /dev/null -b y -cwd -V"

# run python command that will setup the spark cluster and launch the provided workflow 
jt.remoteCommand = "spark_launch"
jt.args = sys.argv[2:]

# run on only one slot
jobid = s.runJob(jt)

