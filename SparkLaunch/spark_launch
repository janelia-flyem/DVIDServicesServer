#!/usr/bin/env python

"""
Usage: prog <service-name> <log-location> <callback>

Actions:
    1.  Provides spark callback address to launching server (via server callback)
    2.  Initializes spark cluster
    3.  Runs job on server (blocking)
    4.  Examines outputs for errors and informs callback

Assumptions: The environment should be properly set by the driver.
"""

# python path must already be specified in PATH

import os
import socket
import sys
import subprocess
import time
import requests
import tempfile
import json

master_output = ""
successful = True

json_header = {'content-type': 'app/json'}

try:
    # ***** Custom Parameters ******
    # location of spark build (CHANGE AS NEEDED!!)
    SPARK_HOME = "/usr/local/spark-current"

    # location of DVIDSparkServices binaries (CHANGE AS NEEDED!!)
    DVIDSPARK_PATH = "/groups/scheffer/home/plazas/development/buildem_sparkcluster/bin"

    # workflow launcher path (CHANGE AS NEEDED!!)
    WORKFLOW_LAUNCHER = "/groups/scheffer/home/plazas/development/DVIDSparkServices/workflows/launchworkflow.py"
    
    # spark configuration path (disable default python) (CHANGE AS NEEDED!!)
    CONF_DIR = "/groups/scheffer/home/plazas/development/DVIDSparkServices/conf"
    
    # set custom library paths (CHANGE AS NEEDED!!)
    os.environ["LD_LIBRARY_PATH"] = "/usr/local/gcc/lib64"
   
    # ***** Derived Parameters *****
    # Add directories to PATH
    PATH_DIRS = DVIDSPARK_PATH + ":" + SPARK_HOME + "/bin:" + SPARK_HOME + "/sbin"

    # set path
    curr_path = os.environ["PATH"]
    os.environ["PATH"] = PATH_DIRS + ":" + curr_path

    # set spark path
    os.environ["SPARK_HOME"] = SPARK_HOME

    # set configuration directory
    os.environ["SPARK_CONF_DIR"] = CONF_DIR

    # set exact python to be used
    os.environ["PYSPARK_PYTHON"] = DVIDSPARK_PATH + "/python"

    # ******** Launch Spark Cluster *******
    HOSTNAME = socket.gethostname()
    master_output = subprocess.check_output(['/usr/local/spark-current/sbin/start-master.sh'])
    
    # this sets default master -- does not need to be specified on commandline
    os.environ["MASTER"] = "spark://" + HOSTNAME + ":7077"

    master_output = master_output + "\n" + os.environ["MASTER"]
    

    # ******** Start Job ********

    # write-back callback address
    status = {}
    status["sparkAddr"] = HOSTNAME + ":4040"
    status["job_status"] = "Running"
    status_str = json.dumps(status)

    requests.post(sys.argv[3], data=status_str, headers=json_header)

    # grab config data
    req = requests.get(sys.argv[3])
    job_data = req.json()
    config_data = job_data["config"]

    # create temporary file for configuration 
    tmpfile = tempfile.NamedTemporaryFile(delete=False)
    tmpfile.write(json.dumps(config_data))
    config_location = tmpfile.name
    tmpfile.close()

    # wait a few seconds for the cluster to stabilise (is this necessary?)
    time.sleep(10)

    # call workflow and wait
    try:
        job_output = subprocess.check_output(['spark-submit', '--master', 'local[4]', WORKFLOW_LAUNCHER, sys.argv[1], '-c', config_location])
    except subprocess.CalledProcessError as e:
        job_output += str(e)
        successful = False

    # remove temporary file
    os.remove(config_location)


    master_output = master_output + "\n" + job_output
except Exception as e:
    master_output += str(sys.exc_info()[0])
    successful = False

# write status and message
status = {}
status["sparkAddr"] = ""
if not successful:
    status["job_status"] = "Error"
    status["job_message"] = master_output
else:
    status["job_status"] = "Finished"
    status["job_message"] = master_output
status_str = json.dumps(status)
requests.post(sys.argv[3], data=status_str, headers=json_header)

# write to file
fout = open(sys.argv[2]+'.log', 'w')
fout.write(master_output)

