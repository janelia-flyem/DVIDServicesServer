#!/usr/bin/env python

"""
Usage: prog [--kill-master-on-exit] <workflow name> <job name> <config file or callback address> <spark services launch-workflow script> <python interpreter location>

If the callback address does not start with an 'http' it is assumed to be a configuration path and no callback will be used

Actions:
    1.  Provides spark callback address to launching server (via server callback)
    2.  Initializes spark cluster
    3.  Runs job on server (blocking)
    4.  Examines outputs for errors and informs callback

Assumptions:
    - The environment should be properly set by the driver.
    - The master node must already be running and specified in the MASTER environment variable.

"""
from __future__ import print_function
import os
import re
import socket
import sys
import subprocess
import traceback
import argparse
import time
import requests
import tempfile
import json
from StringIO import StringIO

DRIVER_HOSTNAME = socket.gethostname()
SPARK_HOME = os.environ["SPARK_HOME"]

# directory location of log file
#LOG_DIR = os.path.expanduser('~') + "/sparkjoblogs"
LOG_DIR = '/groups/flyem/data/scratchspace/sparkjoblogs'


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--kill-master-on-exit', action='store_true')
    parser.add_argument('workflow_name')
    parser.add_argument('job_name')
    parser.add_argument('config_or_callback_address')
    parser.add_argument('launch_workflow_script')
    parser.add_argument('pyspark_python_interpreter')
    args = parser.parse_args()
    
    # Master node must already be running and specified in the environment.
    if 'MASTER' not in os.environ:
        sys.stderr.write('Error: MASTER environment variable not set!\n')
        sys.exit(1)
    
    if not re.match("spark://.+:[0-9]+", os.environ['MASTER']):
        sys.stderr.write('MASTER environment variable is invalid:\n'
                         'MASTER="{}"\n'.format(os.environ['MASTER']))
        sys.exit(1)
    
    MASTER = os.environ['MASTER']

    if args.kill_master_on_exit:
        if 'MASTER_BJOB_ID' not in os.environ:
            sys.stderr.write('Error: MASTER_BJOB_ID environment variable not set!\n')
            sys.exit(1)
        MASTER_BJOB_ID = os.environ['MASTER_BJOB_ID']        
    
    ##### CUSTOM CONFIGURATION ###########
        
    driver_output = StringIO()
    successful = False
    
    json_header = {'content-type': 'app/json'}
    start = time.time()
    
    try:
        # ******** Start Job ********
        configfile = args.config_or_callback_address
    
        hascallback = args.config_or_callback_address.startswith("http")        
        if hascallback:
            # write-back callback address
            status = {}
            status["sparkAddr"] = DRIVER_HOSTNAME
            status["job_status"] = "Running"
            status_str = json.dumps(status)
    
            requests.post(args.config_or_callback_address, data=status_str, headers=json_header)
    
            configfile = configfile + "/config"
    
        # call workflow and wait
        try:
            job_output = subprocess.check_output([SPARK_HOME + '/bin/spark-submit',
                                                  args.launch_workflow_script,
                                                  args.workflow_name,
                                                  '-c',
                                                  configfile])
            driver_output.write( job_output + "\n" )
            successful = True
        finally:
            time.sleep(20)
    
    except subprocess.CalledProcessError as ex:
        traceback.print_exc(file=driver_output)
        driver_output.write(ex.output)    
        driver_output.write("Subprocess return code: {}\n".format(ex.returncode) )
        raise    
    
    except:
        traceback.print_exc(file=driver_output)
        raise
    
    finally:
        # record time
        finish = time.time()
        driver_output.write( "\n" + "Total Time: " + str(finish-start) + "\n" )
        
        # write status and message
        status = {}
        status["sparkAddr"] = ""
        if not successful:
            status["job_status"] = "Error"
        else:
            status["job_status"] = "Finished"
        status["job_message"] = driver_output.getvalue()
        status_str = json.dumps(status)
        
        if hascallback:
            requests.post(args.config_or_callback_address, data=status_str, headers=json_header)
        
        # write to file
        with open(LOG_DIR + "/" + args.job_name +'.log', 'w') as fout:
            fout.write(driver_output.getvalue() + "\n")
            fout.write( "Launch script done: {}\n".format( {True: "successful", False: "UNSUCCESSFUL"}[successful] ) )

        if args.kill_master_on_exit:
            # Kill the spark cluster
            print('Killing Spark Master (JOB_ID={})'.format(MASTER_BJOB_ID))
            subprocess.check_call('bkill {}'.format(MASTER_BJOB_ID), shell=True)

if __name__ == "__main__":
    main()
